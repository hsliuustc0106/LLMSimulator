attn_config:
  type: "MLA"
  d_model: 7168
  num_attention_heads: 128
  head_dim: 56
  num_key_value_heads: 128
  v_head_dim: 128
  attention_dropout: 0.0
  attention_bias: false
  max_position_embeddings: 163840
  rope_theta: 10000
  qk_rope_head_dim: 64
  qk_nope_head_dim: 128


ffn_config:
  d_model: 7168                    # hidden_size
  d_ff: 18432                      # intermediate_size
  hidden_act: silu
  initializer_range: 0.02
  rms_norm_eps: 1e-06
  first_k_dense_replace: 3