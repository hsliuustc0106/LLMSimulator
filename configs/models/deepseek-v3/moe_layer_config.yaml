attn_config:
  type: "MLA"
  d_model: 7168
  num_attention_heads: 128
  head_dim: 56
  num_key_value_heads: 128
  v_head_dim: 128
  attention_dropout: 0.0
  attention_bias: false
  max_position_embeddings: 163840
  rope_theta: 10000
  qk_rope_head_dim: 64
  qk_nope_head_dim: 128

moe_config:
  moe_intermediate_size: 2048
  num_experts_per_tok: 8
  n_routed_experts: 256
  n_shared_experts: 1
  n_group: 8
  topk_group: 4
  topk_method: noaux_tc
  num_nextn_predict_layers: 1
  routed_scaling_factor: 2.5

